{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jMIBikVVMBN",
        "outputId": "03b566e6-8b6b-4392-ff11-dc90299311d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\ngmin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Data Manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Text Processing and NLP\n",
        "import emoji\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Machine Learning\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report, accuracy_score, silhouette_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download(\"vader_lexicon\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqKuQ4dapK4U"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lv4YfK4ivCB1"
      },
      "outputs": [],
      "source": [
        "df_sw = pd.read_csv(\"00_dataset/with_stopwords/cleaned_reviews_sw.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "glGNkOA6zWM-",
        "outputId": "4bc1984d-438a-44b2-fc18-8afca4685357"
      },
      "outputs": [],
      "source": [
        "df_sw.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul5zEA9Uuaa3"
      },
      "source": [
        "## Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wITU9Ft0VLZ",
        "outputId": "348f38dc-ae6f-41c1-d97e-49492b7e669f"
      },
      "outputs": [],
      "source": [
        "X = df_sw[\"cleaned_text_sw\"]\n",
        "y = df_sw[\"label\"]\n",
        "\n",
        "# First, split into Train (80%) and Test (20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Next, split Train (80%) into Train (70%) and Validation (10%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, stratify=y_train, random_state=42)\n",
        "\n",
        "print(\"Training set: \", len(X_train))\n",
        "print(\"Test set: \", len(X_test))\n",
        "print(\"Validation set: \", len(X_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO0G1Q0O1Q--",
        "outputId": "3c40a34f-018a-4974-8cd7-672d48020bd6"
      },
      "outputs": [],
      "source": [
        "# Check class distribution after split\n",
        "print(\"Train Class Distribution:\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"\\nValidation Class Distribution:\\n\", y_val.value_counts(normalize=True))\n",
        "print(\"\\nTest Class Distribution:\\n\", y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1fPH3dHu_Ws"
      },
      "source": [
        "# Feature Extraction - With Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04qThEmT2eRx"
      },
      "outputs": [],
      "source": [
        "vectorizer = TfidfVectorizer(max_features=5000)  # Top 5000 words\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "X_test_tfidf = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAYLxnHhztHg"
      },
      "source": [
        "# Baseline Model - With Stopwords\n",
        "\n",
        "Baseline model no need to perform feature engineering. This will be a simple benchmark with \"raw\" processed data.\n",
        "\n",
        "[Start simple](https://https://lopezyse.medium.com/your-guide-to-feature-engineering-383f7e8b7584#:~:text=Start%20simple%3A%20you%20can%20get,features%20to%20engineer%20and%20use.): you can get an excellent baseline performance by creating an initial model without deep features. After this baseline is achieved, you can try more esoteric approaches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "7G6m0AEK1Kp9",
        "outputId": "18134934-5ca3-4924-bdc5-712ceb6a85cb"
      },
      "outputs": [],
      "source": [
        "nb_baseline = MultinomialNB()\n",
        "\n",
        "nb_baseline.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTfW7ezz1-Y-",
        "outputId": "74b14917-d343-4e90-eb97-d2e6d0db848c"
      },
      "outputs": [],
      "source": [
        "# Predict on test set\n",
        "yPred_nbBaseline = nb_baseline.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate Baseline Performance\n",
        "print(\"Accuracy: \", accuracy_score(y_test, yPred_nbBaseline))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, yPred_nbBaseline))\n",
        "\n",
        "# Compute ROC-AUC\n",
        "yProba_nbBaseline = nb_baseline.predict_proba(X_test_tfidf)[:, 1]\n",
        "print(\"ROC-AUC Score: \", roc_auc_score(y_test, yProba_nbBaseline))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "bKNB4GxI6USk",
        "outputId": "c5480377-1f4e-4e26-d84b-6b1acbfc9b9e"
      },
      "outputs": [],
      "source": [
        "# Generate confusion matrix\n",
        "cm_baseline = confusion_matrix(y_test, yPred_nbBaseline)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_baseline, display_labels=[0, 1])\n",
        "disp.plot(cmap=\"Blues\")  # Choose a color map for visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_kz_mT27IDl"
      },
      "source": [
        "## Analysis of Baseline Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frjRnZCD3ZDc"
      },
      "source": [
        "* The model shows an accuracy of 86.8%.\n",
        "* Precision for Class 1 is 0.62. This shows that when the model predicts as Spam, it is correct 62% of the time.\n",
        "* ROC-AUC score of 0.718 suggests the model is somewhat distinguishing between classes.\n",
        "---\n",
        "* Recall for Class 1 is 0. This shows that the model is failing to detect Spam.\n",
        "  <br>(Recall is crucial in imbalanced datasets)\n",
        "* F1-score for class is 0. This confirms that the model is not capturing the minority class at all.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvb65aRr7kjc"
      },
      "source": [
        "## Class imbalance handling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWWugX5F7q9s"
      },
      "source": [
        "Class Balancing with Naive Balance: to check if doing so will improve the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "uMWiziPd7-UO",
        "outputId": "14d97096-c740-4d8a-ddc6-e0f42ded0f9c"
      },
      "outputs": [],
      "source": [
        "nb_balanced = MultinomialNB(class_prior=[0.5, 0.5]) # Giving equal weightage to both classes\n",
        "nb_balanced.fit(X_train_tfidf, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "4X9cIsVn8OQx",
        "outputId": "b5bb1d41-9dd8-4a16-c142-c7121a4659e4"
      },
      "outputs": [],
      "source": [
        "y_pred_balanced = nb_balanced.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate Baseline Performance\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred_balanced))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_balanced))\n",
        "\n",
        "# Compute ROC-AUC\n",
        "y_proba_balanced = nb_balanced.predict_proba(X_test_tfidf)[:, 1]\n",
        "print(\"ROC-AUC Score: \", roc_auc_score(y_test, y_proba_balanced))\n",
        "\n",
        "cm_balanced = confusion_matrix(y_test, y_pred_balanced)\n",
        "disp_balanced = ConfusionMatrixDisplay(confusion_matrix=cm_balanced, display_labels=[0, 1])\n",
        "disp_balanced.plot(cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nETCHkff9zh3"
      },
      "source": [
        "## Apply SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQjCMSRR9271"
      },
      "outputs": [],
      "source": [
        "# Apply SMOTE\n",
        "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, y_train)\n",
        "\n",
        "# Train Naive Bayes on resampled data\n",
        "nb_smote = MultinomialNB()\n",
        "nb_smote.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Predict using the SMOTE-trained model\n",
        "y_pred_smote = nb_smote.predict(X_test_tfidf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "7IAiwjcj-Jdo",
        "outputId": "b6e73dfe-3d78-445a-bbe8-dcf2b72b4a57"
      },
      "outputs": [],
      "source": [
        "# Evaluate Baseline Performance\n",
        "print(\"Accuracy: \", accuracy_score(y_test, y_pred_smote))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_smote))\n",
        "\n",
        "# Compute ROC-AUC\n",
        "y_proba_smote = nb_smote.predict_proba(X_test_tfidf)[:, 1]\n",
        "print(\"ROC-AUC Score: \", roc_auc_score(y_test, y_proba_smote))\n",
        "\n",
        "cm_smote = confusion_matrix(y_test, y_pred_smote)\n",
        "disp_smote = ConfusionMatrixDisplay(confusion_matrix=cm_smote, display_labels=[0, 1])\n",
        "disp_smote.plot(cmap=\"Blues\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12aFwYPNBcaY"
      },
      "source": [
        "## Baseline Model Performance Comparison - Naive Bayes\n",
        "\n",
        "| Model Version  | Accuracy | Precision (Class 1) | Recall (Class 1) | F1-Score (Class 1) | ROC-AUC Score |\n",
        "|---------------|----------|--------------------|-----------------|-----------------|---------------|\n",
        "| **Initial Baseline** | 86.8% | 62% | 0%  | 0%  | 0.718 |\n",
        "| **Balanced Classes** | 64.8% | 22% | 68% | 34% | 0.718 |\n",
        "| **SMOTE** | 65.7% | 23% | 67% | 34% | 0.719 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6QvWrKIFRmv"
      },
      "source": [
        "# Feature Engineering\n",
        "\n",
        "[**Feature engineering**](https://medium.com/data-and-beyond/a-step-to-step-guide-for-feature-engineering-over-textual-data-nlp-a54c53081e04) is the process of selecting and creating the most relevant and useful features to input into a machine learning model. It is a crucial step in the machine learning process that can significantly impact the model’s performance, complexity, and ability to generalize to new data. By carefully selecting and constructing the features used as input, it is possible to improve the accuracy and effectiveness of the model and avoid overfitting.\n",
        "\n",
        "\n",
        "1. Length of Tweet\n",
        "  \n",
        "  The length of the tweet (in characters) can be a useful feature, as longer tweets may contain more information than shorter tweets. We can extract the length of the tweet like this:\n",
        "  ```\n",
        "  def get_length(text):\n",
        "    return len(text)\n",
        "  ```\n",
        "\n",
        "2. Number of Exclamation Points\n",
        "\n",
        "  The number of exclamation points in a tweet can be a useful feature, as it may indicate the intensity or emotion of the tweet. We can extract the number of exclamation points like this:\n",
        "  ```\n",
        "  def get_exclamation_count(text):\n",
        "    return text.count('!')\n",
        "  ```\n",
        "3. Number of Question Marks\n",
        "\n",
        "  Similar to exclamation points, the number of question marks in a tweet can be a useful feature. We can extract the number of question marks like this:\n",
        "  ```\n",
        "  def get_question_mark_count(text):\n",
        "    return text.count('?')\n",
        "  ```\n",
        "4. Sentiment\n",
        "\n",
        "  The sentiment of a tweet (i.e., whether it is positive, negative, or neutral) can be a useful feature for tasks such as sentiment analysis. We can use a sentiment analysis tool to extract the sentiment of a tweet. One such tool is SentiStrength, which can be used in Python like this:\n",
        "\n",
        "  ```\n",
        "  import sentistrength\n",
        "\n",
        "  def get_sentiment(text):\n",
        "    sentiment = sentistrength.analyze(text)[0]\n",
        "    return sentiment\n",
        "  ```\n",
        "\n",
        "5. Named Entities\n",
        "\n",
        "  Named entities are specific people, organizations, locations, etc. that are mentioned in a tweet. We can use a named entity recognition tool to extract the named entities from a tweet. Here’s an example using spaCy:\n",
        "\n",
        "  ```\n",
        "  import spacy\n",
        "\n",
        "  # Load spaCy model\n",
        "  nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "  def get_named_entities(text):\n",
        "    doc = nlp(text)\n",
        "    named_entities = [X.text for X in doc.ents]\n",
        "    return named_entities\n",
        "  ```\n",
        "\n",
        "6. Emoji\n",
        "\n",
        "  Emojis can convey emotion and sentiment in a tweet, and can therefore be a useful feature. We can use the emoji library to extract the emoji from a tweet like this:\n",
        "\n",
        "  ```\n",
        "  import emoji\n",
        "\n",
        "  def get_emojis(text):\n",
        "    emojis = [emoji.emojize(word) for word in text.split()]\n",
        "    emojis = [emoji for emoji in emojis if emoji != word]\n",
        "    return emojis\n",
        "  ```\n",
        "\n",
        "7. URL (Already removed in cleaned text)\n",
        "\n",
        "  If a tweet contains a URL, it can be a useful feature for tasks such as topic classification or web scraping. We can extract the URL from a tweet using a regular expression like this:\n",
        "\n",
        "  ```\n",
        "  def get_url(text):\n",
        "    url = re.findall(r'https?://\\S+', text)\n",
        "    return url\n",
        "  ```\n",
        "\n",
        "8. Word Count\n",
        "\n",
        "  The word count of a tweet can be a useful feature, as it may give us an indication of the complexity or length of the tweet. We can get the word count like this:\n",
        "\n",
        "  ```\n",
        "  def get_word_count(text):\n",
        "    return len(text.split())\n",
        "  ```\n",
        "\n",
        "9. Average Word Length\n",
        "\n",
        "  The average length of the words in a tweet can be a useful feature. We can compute the average word length like this:\n",
        "\n",
        "  ```\n",
        "  def get_avg_word_length(text):\n",
        "    words = text.split()\n",
        "    word_count = len(words)\n",
        "    avg_word_length = sum(len(word) for word in words) / word_count\n",
        "    return avg_word_length\n",
        "```\n",
        "10. Capitalized Words\n",
        "\n",
        "  Words that are capitalized in a tweet may be more important or relevant than lowercase words. We can extract the capitalized words from a tweet like this:\n",
        "\n",
        "  ```\n",
        "  def get_capitalized_words(text):\n",
        "    capitalized_words = [word for word in text.split() if word.isupper()]\n",
        "    return capitalized_words\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7WkBMeLFROI"
      },
      "outputs": [],
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def extract_features(text):\n",
        "    sentiment_score = TextBlob(text).sentiment.polarity  # Sentiment (-1 to +1)\n",
        "    vader_score = sia.polarity_scores(text)[\"compound\"]  # VADER sentiment\n",
        "    review_length = len(text.split())  # Word count\n",
        "    exclamation_count = text.count(\"!\")  # Number of !\n",
        "    question_count = text.count(\"?\")  # Number of ?\n",
        "    uppercase_ratio = sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0  # % uppercase\n",
        "    duplicate_word_count = len([w for w in text.split() if text.split().count(w) > 1])  # Repeated words\n",
        "    emoji_count = len([char for char in text if char in emoji.EMOJI_DATA])  # Count emojis\n",
        "    avg_word_length = sum(len(word) for word in text.split()) / review_length if review_length > 0 else 0  # Avg word length\n",
        "\n",
        "    return [sentiment_score, vader_score, review_length, exclamation_count, question_count, uppercase_ratio,\n",
        "            duplicate_word_count, emoji_count, avg_word_length]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnwVWmBHsRtq"
      },
      "outputs": [],
      "source": [
        "# Apply feature extraction to dataset\n",
        "df_train_features = X_train.apply(extract_features)\n",
        "df_test_features = X_test.apply(extract_features)\n",
        "df_val_features = X_val.apply(extract_features)\n",
        "\n",
        "features_cols = [\"sentiment_score\", \"vader_score\", \"review_length\",\n",
        "                 \"exclamation_count\", \"question_count\", \"uppercase_ratio\",\n",
        "                 \"duplicate_word_count\", \"emoji_count\", \"avg_word_length\"]\n",
        "\n",
        "df_train_features = pd.DataFrame(df_train_features.tolist(), columns=features_cols)\n",
        "df_test_features = pd.DataFrame(df_test_features.tolist(), columns=features_cols)\n",
        "df_val_features = pd.DataFrame(df_val_features.tolist(), columns=features_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOd-rlmTvAeR"
      },
      "outputs": [],
      "source": [
        "# Convert sparse TF-IDF matrix to dense NumPy array\n",
        "X_train_tfidf_dense = X_train_tfidf.toarray()\n",
        "X_test_tfidf_dense = X_test_tfidf.toarray()\n",
        "X_val_tfidf_dense = X_val_tfidf.toarray()\n",
        "\n",
        "# Convert TF-IDF dense matrix to DataFrame\n",
        "df_train_tfidf = pd.DataFrame(X_train_tfidf_dense)\n",
        "df_test_tfidf = pd.DataFrame(X_test_tfidf_dense)\n",
        "df_val_tfidf = pd.DataFrame(X_val_tfidf_dense)\n",
        "\n",
        "# **Check if row counts match before merging**\n",
        "assert df_train_tfidf.shape[0] == df_train_features.shape[0], \"Mismatch in train set sizes!\"\n",
        "assert df_test_tfidf.shape[0] == df_test_features.shape[0], \"Mismatch in test set sizes!\"\n",
        "assert df_val_tfidf.shape[0] == df_val_features.shape[0], \"Mismatch in validation set sizes!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reset indices for training set\n",
        "df_train_tfidf.reset_index(drop=True, inplace=True)\n",
        "df_train_features.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Reset indices for testing set\n",
        "df_test_tfidf.reset_index(drop=True, inplace=True)\n",
        "df_test_features.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Reset indices for validation set\n",
        "df_val_tfidf.reset_index(drop=True, inplace=True)\n",
        "df_val_features.reset_index(drop=True, inplace=True)\n",
        "y_val.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stuTSU50zbdT"
      },
      "outputs": [],
      "source": [
        "df_y_train = pd.DataFrame(y_train, columns=[\"label\"])\n",
        "df_y_test = pd.DataFrame(y_test, columns=[\"label\"])\n",
        "df_y_val = pd.DataFrame(y_val, columns=[\"label\"])\n",
        "\n",
        "# Check if the row counts of y labels match the features before merging\n",
        "assert df_train_tfidf.shape[0] == df_y_train.shape[0], \"Mismatch in train labels!\"\n",
        "assert df_test_tfidf.shape[0] == df_y_test.shape[0], \"Mismatch in test labels!\"\n",
        "assert df_val_tfidf.shape[0] == df_y_val.shape[0], \"Mismatch in validation labels!\"\n",
        "\n",
        "# Check if indices match\n",
        "print(df_train_tfidf.index.equals(df_y_train.index))  # Should be True\n",
        "print(df_test_tfidf.index.equals(df_test_features.index))    # Should be True\n",
        "print(df_val_tfidf.index.equals(df_y_val.index))      # Should be True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if indices match\n",
        "print(df_train_tfidf.index.equals(df_train_features.index))  # Should be True\n",
        "print(df_test_tfidf.index.equals(df_y_test.index))    # Should be True\n",
        "print(df_val_tfidf.index.equals(df_val_features.index))      # Should be True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_y_val.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhuQZa36wvmG"
      },
      "outputs": [],
      "source": [
        "# Merge TF-IDF with extracted features\n",
        "df_train_combined = pd.concat([df_train_tfidf, df_train_features, df_y_train], axis=1)\n",
        "df_test_combined = pd.concat([df_test_tfidf, df_test_features, df_y_test], axis=1)\n",
        "df_val_combined = pd.concat([df_val_tfidf, df_val_features, df_y_val], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"NaN values in train set:\\n\", df_train_combined.isna().sum())\n",
        "print(\"NaN values in test set:\\n\", df_test_combined.isna().sum())\n",
        "print(\"NaN values in validation set:\\n\", df_val_combined.isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train_combined.head()\n",
        "df_test_combined.head()\n",
        "df_val_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6nZiN-IHnPt"
      },
      "outputs": [],
      "source": [
        "df_train_combined.to_csv(\"00_dataset/with_stopwords/train_features_sw.csv\", index=False)\n",
        "df_test_combined.to_csv(\"00_dataset/with_stopwords/test_features_sw.csv\", index=False)\n",
        "df_val_combined.to_csv(\"00_dataset/with_stopwords/val_features_sw.csv\", index=False)\n",
        "\n",
        "print(\"Features saved successfully to CSV!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "5LAdCfvbsB2Q",
        "kN6Ff2xnpcYW",
        "BRw6BrTTtID5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
