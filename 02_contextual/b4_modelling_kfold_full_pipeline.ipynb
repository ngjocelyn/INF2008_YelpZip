{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import balanced_accuracy_score, matthews_corrcoef, precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>prod_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>date</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5044</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-16</td>\n",
       "      <td>Drinks were bad, the hot chocolate was watered...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5045</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-09-08</td>\n",
       "      <td>This was the worst experience I've ever had a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5046</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2013-10-06</td>\n",
       "      <td>This is located on the site of the old Spruce ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5047</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-11-30</td>\n",
       "      <td>I enjoyed coffee and breakfast twice at Toast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5048</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>2014-08-28</td>\n",
       "      <td>I love Toast! The food choices are fantastic -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608593</th>\n",
       "      <td>119664</td>\n",
       "      <td>5039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-20</td>\n",
       "      <td>When I first moved to the area I must say I wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608594</th>\n",
       "      <td>56277</td>\n",
       "      <td>5039</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-11-12</td>\n",
       "      <td>Kind of pricey. I guess I expected a ridiculou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608595</th>\n",
       "      <td>265320</td>\n",
       "      <td>5039</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-08-22</td>\n",
       "      <td>Stopped by this restaurant yesterday, we just ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608596</th>\n",
       "      <td>161722</td>\n",
       "      <td>5039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2011-05-11</td>\n",
       "      <td>Finally checked out The Best Subs in Claremont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608597</th>\n",
       "      <td>78454</td>\n",
       "      <td>5039</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2010-07-17</td>\n",
       "      <td>Just got me some \"Best Subs\" and I gotta say, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>608458 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  prod_id  rating  label        date  \\\n",
       "0          5044        0     1.0     -1  2014-11-16   \n",
       "1          5045        0     1.0     -1  2014-09-08   \n",
       "2          5046        0     3.0     -1  2013-10-06   \n",
       "3          5047        0     5.0     -1  2014-11-30   \n",
       "4          5048        0     5.0     -1  2014-08-28   \n",
       "...         ...      ...     ...    ...         ...   \n",
       "608593   119664     5039     4.0      1  2013-01-20   \n",
       "608594    56277     5039     2.0      1  2012-11-12   \n",
       "608595   265320     5039     1.0      1  2012-08-22   \n",
       "608596   161722     5039     4.0      1  2011-05-11   \n",
       "608597    78454     5039     4.0      1  2010-07-17   \n",
       "\n",
       "                                                   review  \n",
       "0       Drinks were bad, the hot chocolate was watered...  \n",
       "1       This was the worst experience I've ever had a ...  \n",
       "2       This is located on the site of the old Spruce ...  \n",
       "3       I enjoyed coffee and breakfast twice at Toast ...  \n",
       "4       I love Toast! The food choices are fantastic -...  \n",
       "...                                                   ...  \n",
       "608593  When I first moved to the area I must say I wa...  \n",
       "608594  Kind of pricey. I guess I expected a ridiculou...  \n",
       "608595  Stopped by this restaurant yesterday, we just ...  \n",
       "608596  Finally checked out The Best Subs in Claremont...  \n",
       "608597  Just got me some \"Best Subs\" and I gotta say, ...  \n",
       "\n",
       "[608458 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../00_dataset/YelpZip/metadata', \n",
    "                 sep='\\t',\n",
    "                 header=None,\n",
    "                 names=[\"user_id\", \"prod_id\", \"rating\", \"label\", \"date\"])\n",
    "reviews_df = pd.read_csv('../00_dataset/YelpZip/reviewContent',\n",
    "                sep='\\t',\n",
    "                header=None,\n",
    "                names=['user_id', 'prod_id', 'date', 'review'])\n",
    "\n",
    "df = df.merge(reviews_df,\n",
    "              left_on=['user_id', 'prod_id', 'date'],\n",
    "              right_on=['user_id', 'prod_id', 'date'],\n",
    "              how='left')\n",
    "df = df.dropna(subset=['review'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id    0\n",
       "prod_id    0\n",
       "rating     0\n",
       "label      0\n",
       "date       0\n",
       "review     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['label'].replace({1: 0, -1: 1})\n",
    "y = df['label']\n",
    "X = df.drop('label', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=42, stratify=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "X_train: (425920, 5)\n",
      "X_validation: (60846, 5)\n",
      "X_test: (121692, 5)\n",
      "y_train: (425920,)\n",
      "y_validation: (60846,)\n",
      "y_test: (60846,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "X_train: {X_train.shape}\n",
    "X_validation: {X_val.shape}\n",
    "X_test: {X_test.shape}\n",
    "y_train: {y_train.shape}\n",
    "y_validation: {y_val.shape}\n",
    "y_test: {y_test.shape}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR: LogisticRegression\n",
      "NN: MLPClassifier\n",
      "KNN: KNeighborsClassifier\n",
      "DT: DecisionTreeClassifier\n",
      "RF: RandomForestClassifier\n",
      "AB: AdaBoostClassifier\n",
      "XGB: XGBClassifier\n",
      "NB: GaussianNB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier  # Boosting\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"NN\": MLPClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"AB\": AdaBoostClassifier(),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"NB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Example usage: print model names\n",
    "for category, model in models.items():\n",
    "    print(f\"{category}: {model.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print what the pipeline does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Pipeline: FSS ===\n",
      "- Uses **feature-engineered dataset**\n",
      "- Scaling: StandardScaler (FSS) or MinMaxScaler (FMS)\n",
      "- Feature selection applied for each method below:\n",
      "  - Feature Selection: baseline\n",
      "    - Columns dropped: None\n",
      "  - Feature Selection: MI\n",
      "    - Columns dropped: ['avg_rating_for_restaurant', 'date', 'extreme_rating_index', 'rating_min', 'review_frequency_for_restaurant', 'std_dev_rating_for_restaurant', 'total_reviews_for_restaurant', 'user_earliest', 'user_latest']\n",
      "  - Feature Selection: Lasso_MI\n",
      "    - Columns dropped: ['avg_rating_for_restaurant', 'date', 'extreme_rating_index', 'rating_min', 'review_frequency_for_restaurant', 'std_dev_rating_for_restaurant', 'total_reviews_for_restaurant', 'user_earliest', 'user_latest', 'user_restaurants_reviewed']\n",
      "  - Feature Selection: RFE\n",
      "    - Columns dropped: ['extreme_rating_index', 'median_rating_for_restaurant', 'rating', 'rating_max', 'rating_min', 'rating_std', 'user_active_percentage', 'user_days_active', 'user_restaurants_reviewed']\n",
      "  - Feature Selection: Lasso_RFE\n",
      "    - Columns dropped: ['extreme_rating_index', 'median_rating_for_restaurant', 'rating', 'rating_max', 'rating_min', 'rating_std', 'user_active_percentage', 'user_days_active', 'user_earliest', 'user_restaurants_reviewed']\n",
      "  - Feature Selection: RFECV\n",
      "    - Columns dropped: ['median_rating_for_restaurant', 'rating', 'rating_max']\n",
      "  - Feature Selection: Lasso_RFECV\n",
      "    - Columns dropped: ['median_rating_for_restaurant', 'rating', 'rating_max', 'user_earliest', 'user_restaurants_reviewed']\n",
      "\n",
      "=== Pipeline: FMS ===\n",
      "- Uses **feature-engineered dataset**\n",
      "- Scaling: StandardScaler (FSS) or MinMaxScaler (FMS)\n",
      "- Feature selection applied for each method below:\n",
      "  - Feature Selection: baseline\n",
      "    - Columns dropped: None\n",
      "  - Feature Selection: MI\n",
      "    - Columns dropped: ['avg_rating_for_restaurant', 'date', 'extreme_rating_index', 'rating_min', 'review_frequency_for_restaurant', 'std_dev_rating_for_restaurant', 'total_reviews_for_restaurant', 'user_earliest', 'user_latest']\n",
      "  - Feature Selection: Lasso_MI\n",
      "    - Columns dropped: ['avg_rating_for_restaurant', 'date', 'extreme_rating_index', 'rating_min', 'review_frequency_for_restaurant', 'std_dev_rating_for_restaurant', 'total_reviews_for_restaurant', 'user_earliest', 'user_latest', 'user_restaurants_reviewed']\n",
      "  - Feature Selection: RFE\n",
      "    - Columns dropped: ['extreme_rating_index', 'median_rating_for_restaurant', 'rating', 'rating_max', 'rating_min', 'rating_std', 'user_active_percentage', 'user_days_active', 'user_restaurants_reviewed']\n",
      "  - Feature Selection: Lasso_RFE\n",
      "    - Columns dropped: ['extreme_rating_index', 'median_rating_for_restaurant', 'rating', 'rating_max', 'rating_min', 'rating_std', 'user_active_percentage', 'user_days_active', 'user_earliest', 'user_restaurants_reviewed']\n",
      "  - Feature Selection: RFECV\n",
      "    - Columns dropped: ['median_rating_for_restaurant', 'rating', 'rating_max']\n",
      "  - Feature Selection: Lasso_RFECV\n",
      "    - Columns dropped: ['median_rating_for_restaurant', 'rating', 'rating_max', 'user_earliest', 'user_restaurants_reviewed']\n",
      "\n",
      "=== Pipeline: SS ===\n",
      "- Uses the dataset **without feature engineering**\n",
      "- Columns dropped: ['user_id', 'prod_id', 'review']\n",
      "- `date` is converted to UNIX timestamp\n",
      "\n",
      "=== Baseline Model ===\n",
      "- Uses **all features**, no feature selection applied\n",
      "- Uses feature-engineered dataset\n",
      "- Scaling: StandardScaler / MinMaxScaler\n"
     ]
    }
   ],
   "source": [
    "# Define drop_columns_dict for feature selection methods\n",
    "drop_columns_dict = {\n",
    "    \"baseline\": set(),\n",
    "    \"MI\": {'user_earliest', 'extreme_rating_index', 'total_reviews_for_restaurant', 'avg_rating_for_restaurant', \n",
    "           'review_frequency_for_restaurant', 'user_latest', 'std_dev_rating_for_restaurant', 'rating_min', 'date'},\n",
    "    \"Lasso_MI\": {'user_earliest', 'extreme_rating_index', 'total_reviews_for_restaurant', 'avg_rating_for_restaurant', \n",
    "                 'review_frequency_for_restaurant', 'user_latest', 'std_dev_rating_for_restaurant', 'rating_min', 'date', \n",
    "                 'user_restaurants_reviewed'},\n",
    "    \"RFE\": {'rating_max', 'median_rating_for_restaurant', 'extreme_rating_index', 'rating', 'user_restaurants_reviewed', \n",
    "            'rating_std', 'user_active_percentage', 'user_days_active', 'rating_min'},\n",
    "    \"Lasso_RFE\": {'rating_max', 'median_rating_for_restaurant', 'extreme_rating_index', 'rating', 'user_restaurants_reviewed', \n",
    "                  'rating_std', 'user_active_percentage', 'user_days_active', 'rating_min', 'user_earliest'},\n",
    "    \"RFECV\": {'rating_max', 'median_rating_for_restaurant', 'rating'},\n",
    "    \"Lasso_RFECV\": {'rating_max', 'median_rating_for_restaurant', 'rating', 'user_earliest', 'user_restaurants_reviewed'}\n",
    "}\n",
    "\n",
    "# Define pipelines\n",
    "pipelines = [\"FSS\", \"FMS\", \"SS\"]\n",
    "\n",
    "# Print out what will be fed into each pipeline\n",
    "for p in pipelines:\n",
    "    print(f\"\\n=== Pipeline: {p} ===\")\n",
    "\n",
    "    if p == \"SS\":\n",
    "        print(\"- Uses the dataset **without feature engineering**\")\n",
    "        print(\"- Columns dropped: ['user_id', 'prod_id', 'review']\")\n",
    "        print(\"- `date` is converted to UNIX timestamp\")\n",
    "    else:\n",
    "        print(\"- Uses **feature-engineered dataset**\")\n",
    "        print(\"- Scaling: StandardScaler (FSS) or MinMaxScaler (FMS)\")\n",
    "        print(\"- Feature selection applied for each method below:\")\n",
    "\n",
    "        for feature_selection_method, drop_columns in drop_columns_dict.items():\n",
    "            print(f\"  - Feature Selection: {feature_selection_method}\")\n",
    "            print(f\"    - Columns dropped: {sorted(drop_columns) if drop_columns else 'None'}\")\n",
    "    \n",
    "print(\"\\n=== Baseline Model ===\")\n",
    "print(\"- Uses **all features**, no feature selection applied\")\n",
    "print(\"- Uses feature-engineered dataset\")\n",
    "print(\"- Scaling: StandardScaler / MinMaxScaler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline (Only Baseline FMS and FSS are saved)\n",
    "<br>\n",
    "Baseline: All Engineered Features\n",
    "<br>\n",
    "MI: Features Selected by Mutual Interest\n",
    "<br>\n",
    "Lasso_MI: MI Features that are lassoed\n",
    "<br>\n",
    "RFE: Features Selected by Recursive Feature Extraction\n",
    "<br>\n",
    "Lasso_RFE: RFE Features that are lassoed\n",
    "<br>\n",
    "RFECV: Features selected by RFECV (RFE Cross Validation)\n",
    "<br>\n",
    "Lasso_RFECV: RFECV Features that are lassoed\n",
    "<br>\n",
    "<br><br>\n",
    "FMS: Features, MinMaxScaler, SMOTE\n",
    "<br>\n",
    "FSS: Features, StandardScaler, SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: LR | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: baseline | Pipeline: FSS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: KNN | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: baseline | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: baseline | Pipeline: FMS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: NN | Feature Selection: baseline | Pipeline: FMS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: KNN | Feature Selection: baseline | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: baseline | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: baseline | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: baseline | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: baseline | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: baseline | Pipeline: FMS ===\n",
      "=== Model: LR | Feature Selection: baseline | Pipeline: SS ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 430, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1383, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '2011-11-13'\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 430, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1383, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '2008-10-13'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m     95\u001b[39m scoring = {\n\u001b[32m     96\u001b[39m     \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     97\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    101\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mmcc\u001b[39m\u001b[33m'\u001b[39m: make_scorer(matthews_corrcoef)\n\u001b[32m    102\u001b[39m }\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m cv_results = \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# Get predicted probabilities for PR-AUC & ROC-AUC\u001b[39;00m\n\u001b[32m    108\u001b[39m y_pred_proba = cross_val_predict(pipeline, X_train, y_train, cv=cv, method=\u001b[33m\"\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:431\u001b[39m, in \u001b[36mcross_validate\u001b[39m\u001b[34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[39m\n\u001b[32m    410\u001b[39m parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n\u001b[32m    411\u001b[39m results = parallel(\n\u001b[32m    412\u001b[39m     delayed(_fit_and_score)(\n\u001b[32m    413\u001b[39m         clone(estimator),\n\u001b[32m   (...)\u001b[39m\u001b[32m    428\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[32m    429\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(scoring):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:517\u001b[39m, in \u001b[36m_warn_or_raise_about_fit_failures\u001b[39m\u001b[34m(results, error_score)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits == num_fits:\n\u001b[32m    511\u001b[39m     all_fits_failed_message = (\n\u001b[32m    512\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    513\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    514\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou can try to debug the error by setting error_score=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    516\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    520\u001b[39m     some_fits_failed_message = (\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe score on these train-test partitions for these parameters\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    526\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    527\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n4 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 430, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1383, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '2011-11-13'\n\n--------------------------------------------------------------------------------\n1 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 518, in fit\n    Xt, yt = self._fit(X, y, routed_params, raw_params=params)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 430, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\imblearn\\pipeline.py\", line 1383, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py\", line 319, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 921, in fit_transform\n    return self.fit(X, y, **fit_params).transform(X)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 894, in fit\n    return self.partial_fit(X, y, sample_weight)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py\", line 930, in partial_fit\n    X = validate_data(\n        ^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 2944, in validate_data\n    out = check_array(X, input_name=\"X\", **check_params)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 1055, in check_array\n    array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py\", line 839, in _asarray_with_order\n    array = numpy.asarray(array, order=order, dtype=dtype)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\pandas\\core\\generic.py\", line 2153, in __array__\n    arr = np.asarray(values, dtype=dtype)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nValueError: could not convert string to float: '2008-10-13'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    balanced_accuracy_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc,\n",
    "    make_scorer\n",
    ")\n",
    "import numpy as np\n",
    "from FeatureEngineer import CombinedEngineer\n",
    "from SmoteTransformer import SMOTETransformer\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"NN\": MLPClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"AB\": AdaBoostClassifier(),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"NB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Feature selection methods with columns to drop\n",
    "drop_columns_dict = {\n",
    "    \"baseline\": set(),\n",
    "    \"MI\": {'user_earliest', 'extreme_rating_index', 'total_reviews_for_restaurant', 'avg_rating_for_restaurant', \n",
    "           'review_frequency_for_restaurant', 'user_latest', 'std_dev_rating_for_restaurant', 'rating_min', 'date'},\n",
    "    \"Lasso_MI\": {'user_earliest', 'extreme_rating_index', 'total_reviews_for_restaurant', 'avg_rating_for_restaurant', \n",
    "                 'review_frequency_for_restaurant', 'user_latest', 'std_dev_rating_for_restaurant', 'rating_min', 'date', \n",
    "                 'user_restaurants_reviewed'},\n",
    "    \"RFE\": {'rating_max', 'median_rating_for_restaurant', 'extreme_rating_index', 'rating', 'user_restaurants_reviewed', \n",
    "            'rating_std', 'user_active_percentage', 'user_days_active', 'rating_min'},\n",
    "    \"Lasso_RFE\": {'rating_max', 'median_rating_for_restaurant', 'extreme_rating_index', 'rating', 'user_restaurants_reviewed', \n",
    "                  'rating_std', 'user_active_percentage', 'user_days_active', 'rating_min', 'user_earliest'},\n",
    "    \"RFECV\": {'rating_max', 'median_rating_for_restaurant', 'rating'},\n",
    "    \"Lasso_RFECV\": {'rating_max', 'median_rating_for_restaurant', 'rating', 'user_earliest', 'user_restaurants_reviewed'}\n",
    "}\n",
    "\n",
    "# Stratified K-Fold setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "prob_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over Feature Selection methods\n",
    "for feature_selection_method, drop_columns in drop_columns_dict.items():\n",
    "    \n",
    "    # Iterate over pipeline types\n",
    "    for p in [\"FSS\", \"FMS\", \"SS\"]:\n",
    "        \n",
    "        # SS only runs once (no feature engineering)\n",
    "        if p == \"SS\" and feature_selection_method != \"baseline\":\n",
    "            continue\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"=== Model: {name} | Feature Selection: {feature_selection_method} | Pipeline: {p} ===\")\n",
    "\n",
    "            if p == \"FSS\":\n",
    "                pipeline = Pipeline([\n",
    "                    ('feature_engineering', CombinedEngineer(drop_columns=list(drop_columns))),  # Feature engineering with column removal\n",
    "                    ('scaler', StandardScaler()),  # Standard Scaling\n",
    "                    ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "                    ('classifier', model)  # Model\n",
    "                ])\n",
    "            elif p == \"SS\":\n",
    "                # Preprocess dataset only once for SS\n",
    "                X_train_copy = X_train.copy()\n",
    "                X_val_copy = X_val.copy()\n",
    "                \n",
    "                X_train_copy['date'] = pd.to_datetime(X_train_copy['date']).astype('int64') // 10**9\n",
    "                X_val_copy['date'] = pd.to_datetime(X_val_copy['date']).astype('int64') // 10**9\n",
    "                X_train_copy = X_train_copy.drop(columns=[\"user_id\", \"prod_id\", \"review\"], errors='ignore')\n",
    "                X_val_copy = X_val_copy.drop(columns=[\"user_id\", \"prod_id\", \"review\"], errors='ignore')\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),  # Standard Scaling\n",
    "                    ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "                    ('classifier', model)  # Model\n",
    "                ])\n",
    "            else:\n",
    "                # FMS (Feature Engineering + MinMaxScaler)\n",
    "                pipeline = Pipeline([\n",
    "                    ('feature_engineering', CombinedEngineer(drop_columns=list(drop_columns))),  # Feature engineering\n",
    "                    ('scaler', MinMaxScaler()),  # MinMax Scaling\n",
    "                    ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "                    ('classifier', model)  # Model\n",
    "                ])\n",
    "\n",
    "            # Define scoring metrics\n",
    "            scoring = {\n",
    "                'accuracy': 'accuracy',\n",
    "                'precision': 'precision',\n",
    "                'recall': 'recall',\n",
    "                'f1': 'f1',\n",
    "                'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "                'mcc': make_scorer(matthews_corrcoef)\n",
    "            }\n",
    "\n",
    "            # Perform cross-validation\n",
    "            cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # Get predicted probabilities for PR-AUC & ROC-AUC\n",
    "            y_pred_proba = cross_val_predict(pipeline, X_train, y_train, cv=cv, method=\"predict_proba\")\n",
    "\n",
    "            # Compute additional metrics\n",
    "            roc_auc_0 = roc_auc_score(y_train, y_pred_proba[:, 0])\n",
    "            roc_auc_1 = roc_auc_score(y_train, y_pred_proba[:, 1])\n",
    "\n",
    "            precision_0, recall_0, _ = precision_recall_curve(y_train, y_pred_proba[:, 0])\n",
    "            pr_auc_0 = auc(recall_0, precision_0)\n",
    "\n",
    "            precision_1, recall_1, _ = precision_recall_curve(y_train, y_pred_proba[:, 1])\n",
    "            pr_auc_1 = auc(recall_1, precision_1)\n",
    "\n",
    "            # Store metrics\n",
    "            results.append({\n",
    "                \"Model\": f\"{name}_{feature_selection_method}_{p}\",\n",
    "                \"Accuracy\": f\"{np.mean(cv_results['test_accuracy']):.4f}\",\n",
    "                \"Precision\": f\"{np.mean(cv_results['test_precision']):.4f}\",\n",
    "                \"Recall\": f\"{np.mean(cv_results['test_recall']):.4f}\",\n",
    "                \"F1 Score\": f\"{np.mean(cv_results['test_f1']):.4f}\",\n",
    "                \"Balanced Accuracy\": f\"{np.mean(cv_results['test_balanced_accuracy']):.4f}\",\n",
    "                \"MCC\": f\"{np.mean(cv_results['test_mcc']):.4f}\",\n",
    "                \"PR-AUC_0\": f\"{pr_auc_0:.4f}\",\n",
    "                \"PR-AUC_1\": f\"{pr_auc_1:.4f}\",\n",
    "                \"ROC-AUC_0\": f\"{roc_auc_0:.4f}\",\n",
    "                \"ROC-AUC_1\": f\"{roc_auc_1:.4f}\"\n",
    "            })\n",
    "\n",
    "# Save results in a single CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"b4_pipeline_comparison_results.csv\", index=False)\n",
    "\n",
    "print(\"All models evaluated and results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save baseline results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b5_pipeline_completed_FSS_FMS.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out only FSS and FMS pipeline results\n",
    "filtered_results = [r for r in results if not r[\"Model\"].endswith(\"_SS\")]\n",
    "\n",
    "# Convert to DataFrame\n",
    "filtered_results_df = pd.DataFrame(filtered_results)\n",
    "\n",
    "# Save results before SS to a new CSV\n",
    "filtered_results_path = \"b4_pipeline_completed_FSS_FMS.csv\"\n",
    "filtered_results_df.to_csv(filtered_results_path, index=False)\n",
    "\n",
    "\n",
    "# Return the saved file path\n",
    "filtered_results_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline for FMS and FSS for Selected Engineered Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: LR | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: KNN | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: MI | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: NN | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: KNN | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: MI | Pipeline: FMS ===\n",
      "=== Model: LR | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: KNN | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: Lasso_MI | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: NN | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: KNN | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: Lasso_MI | Pipeline: FMS ===\n",
      "=== Model: LR | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: KNN | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: RFE | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: NN | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: KNN | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: RFE | Pipeline: FMS ===\n",
      "=== Model: LR | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: KNN | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: Lasso_RFE | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: NN | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: KNN | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: Lasso_RFE | Pipeline: FMS ===\n",
      "=== Model: LR | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: RFECV | Pipeline: FSS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: KNN | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: RFECV | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: NN | Feature Selection: RFECV | Pipeline: FMS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: KNN | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: RFECV | Pipeline: FMS ===\n",
      "=== Model: LR | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: NN | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: KNN | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: DT | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: RF | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: AB | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: XGB | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: NB | Feature Selection: Lasso_RFECV | Pipeline: FSS ===\n",
      "=== Model: LR | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "=== Model: NN | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Documents\\GitHub\\INF2008_YelpZip\\venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: KNN | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "=== Model: DT | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "=== Model: RF | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "=== Model: AB | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "=== Model: XGB | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "=== Model: NB | Feature Selection: Lasso_RFECV | Pipeline: FMS ===\n",
      "All models evaluated and results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    balanced_accuracy_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc,\n",
    "    make_scorer\n",
    ")\n",
    "import numpy as np\n",
    "from FeatureEngineer import CombinedEngineer\n",
    "from SmoteTransformer import SMOTETransformer\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"NN\": MLPClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"AB\": AdaBoostClassifier(),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"NB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Feature selection methods with columns to drop\n",
    "drop_columns_dict = {\n",
    "    \"baseline\": set(),\n",
    "    \"MI\": {'user_earliest', 'extreme_rating_index', 'total_reviews_for_restaurant', 'avg_rating_for_restaurant', \n",
    "           'review_frequency_for_restaurant', 'user_latest', 'std_dev_rating_for_restaurant', 'rating_min', 'date'},\n",
    "    \"Lasso_MI\": {'user_earliest', 'extreme_rating_index', 'total_reviews_for_restaurant', 'avg_rating_for_restaurant', \n",
    "                 'review_frequency_for_restaurant', 'user_latest', 'std_dev_rating_for_restaurant', 'rating_min', 'date', \n",
    "                 'user_restaurants_reviewed'},\n",
    "    \"RFE\": {'rating_max', 'median_rating_for_restaurant', 'extreme_rating_index', 'rating', 'user_restaurants_reviewed', \n",
    "            'rating_std', 'user_active_percentage', 'user_days_active', 'rating_min'},\n",
    "    \"Lasso_RFE\": {'rating_max', 'median_rating_for_restaurant', 'extreme_rating_index', 'rating', 'user_restaurants_reviewed', \n",
    "                  'rating_std', 'user_active_percentage', 'user_days_active', 'rating_min', 'user_earliest'},\n",
    "    \"RFECV\": {'rating_max', 'median_rating_for_restaurant', 'rating'},\n",
    "    \"Lasso_RFECV\": {'rating_max', 'median_rating_for_restaurant', 'rating', 'user_earliest', 'user_restaurants_reviewed'}\n",
    "}\n",
    "\n",
    "# Stratified K-Fold setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "prob_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over Feature Selection methods\n",
    "for feature_selection_method, drop_columns in drop_columns_dict.items():\n",
    "\n",
    "    # Skip baseline \n",
    "    if feature_selection_method == \"baseline\":\n",
    "        continue  \n",
    "\n",
    "    # Iterate over pipeline types (skip SS)\n",
    "    for p in [\"FSS\", \"FMS\"]:\n",
    "\n",
    "\n",
    "        for name, model in models.items():\n",
    "            print(f\"=== Model: {name} | Feature Selection: {feature_selection_method} | Pipeline: {p} ===\")\n",
    "\n",
    "            if p == \"FSS\":\n",
    "                # FSS (Feature Engineering + StandardScaler)\n",
    "                pipeline = Pipeline([\n",
    "                    ('feature_engineering', CombinedEngineer(drop_columns=list(drop_columns))),  # Feature engineering with column removal\n",
    "                    ('scaler', StandardScaler()),  # Standard Scaling\n",
    "                    ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "                    ('classifier', model)  # Model\n",
    "                ])\n",
    "            elif p == \"SS\":\n",
    "                # Preprocess dataset only once for SS\n",
    "                X_train_copy = X_train.copy()\n",
    "                X_val_copy = X_val.copy()\n",
    "                \n",
    "                X_train_copy['date'] = pd.to_datetime(X_train_copy['date']).astype('int64') // 10**9\n",
    "                X_val_copy['date'] = pd.to_datetime(X_val_copy['date']).astype('int64') // 10**9\n",
    "                X_train_copy = X_train_copy.drop(columns=[\"user_id\", \"prod_id\", \"review\"], errors='ignore')\n",
    "                X_val_copy = X_val_copy.drop(columns=[\"user_id\", \"prod_id\", \"review\"], errors='ignore')\n",
    "\n",
    "                pipeline = Pipeline([\n",
    "                    ('scaler', StandardScaler()),  # Standard Scaling\n",
    "                    ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "                    ('classifier', model)  # Model\n",
    "                ])\n",
    "            else:\n",
    "                # FMS (Feature Engineering + MinMaxScaler)\n",
    "                pipeline = Pipeline([\n",
    "                    ('feature_engineering', CombinedEngineer(drop_columns=list(drop_columns))),  # Feature engineering\n",
    "                    ('scaler', MinMaxScaler()),  # MinMax Scaling\n",
    "                    ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "                    ('classifier', model)  # Model\n",
    "                ])\n",
    "\n",
    "            # Define scoring metrics\n",
    "            scoring = {\n",
    "                'accuracy': 'accuracy',\n",
    "                'precision': 'precision',\n",
    "                'recall': 'recall',\n",
    "                'f1': 'f1',\n",
    "                'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "                'mcc': make_scorer(matthews_corrcoef)\n",
    "            }\n",
    "\n",
    "            # Perform cross-validation\n",
    "            cv_results = cross_validate(pipeline, X_train, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "            # Get predicted probabilities for PR-AUC & ROC-AUC\n",
    "            y_pred_proba = cross_val_predict(pipeline, X_train, y_train, cv=cv, method=\"predict_proba\")\n",
    "\n",
    "            # Compute additional metrics\n",
    "            roc_auc_0 = roc_auc_score(y_train, y_pred_proba[:, 0])\n",
    "            roc_auc_1 = roc_auc_score(y_train, y_pred_proba[:, 1])\n",
    "\n",
    "            precision_0, recall_0, _ = precision_recall_curve(y_train, y_pred_proba[:, 0])\n",
    "            pr_auc_0 = auc(recall_0, precision_0)\n",
    "\n",
    "            precision_1, recall_1, _ = precision_recall_curve(y_train, y_pred_proba[:, 1])\n",
    "            pr_auc_1 = auc(recall_1, precision_1)\n",
    "\n",
    "            # Store metrics\n",
    "            results.append({\n",
    "                \"Model\": f\"{name}_{feature_selection_method}_{p}\",\n",
    "                \"Accuracy\": f\"{np.mean(cv_results['test_accuracy']):.4f}\",\n",
    "                \"Precision\": f\"{np.mean(cv_results['test_precision']):.4f}\",\n",
    "                \"Recall\": f\"{np.mean(cv_results['test_recall']):.4f}\",\n",
    "                \"F1 Score\": f\"{np.mean(cv_results['test_f1']):.4f}\",\n",
    "                \"Balanced Accuracy\": f\"{np.mean(cv_results['test_balanced_accuracy']):.4f}\",\n",
    "                \"MCC\": f\"{np.mean(cv_results['test_mcc']):.4f}\",\n",
    "                \"PR-AUC_0\": f\"{pr_auc_0:.4f}\",\n",
    "                \"PR-AUC_1\": f\"{pr_auc_1:.4f}\",\n",
    "                \"ROC-AUC_0\": f\"{roc_auc_0:.4f}\",\n",
    "                \"ROC-AUC_1\": f\"{roc_auc_1:.4f}\"\n",
    "            })\n",
    "\n",
    "# Save results in a single CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"b4_pipeline_comparison_results_rest.csv\", index=False)\n",
    "\n",
    "print(\"All models evaluated and results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Pipeline for SS (No Feature Selection Methods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: LR | Pipeline: SS ===\n",
      "=== Model: NN | Pipeline: SS ===\n",
      "=== Model: KNN | Pipeline: SS ===\n",
      "=== Model: DT | Pipeline: SS ===\n",
      "=== Model: RF | Pipeline: SS ===\n",
      "=== Model: AB | Pipeline: SS ===\n",
      "=== Model: XGB | Pipeline: SS ===\n",
      "=== Model: NB | Pipeline: SS ===\n",
      "SS pipeline models evaluated and results saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold, cross_val_predict\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    balanced_accuracy_score, matthews_corrcoef, roc_auc_score, precision_recall_curve, auc,\n",
    "    make_scorer\n",
    ")\n",
    "import numpy as np\n",
    "from SmoteTransformer import SMOTETransformer\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"LR\": LogisticRegression(),\n",
    "    \"NN\": MLPClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"AB\": AdaBoostClassifier(),\n",
    "    \"XGB\": XGBClassifier(),\n",
    "    \"NB\": GaussianNB(),\n",
    "}\n",
    "\n",
    "# Stratified K-Fold setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "prob_df = pd.DataFrame()\n",
    "\n",
    "# Preprocess dataset for SS\n",
    "X_train_SS = X_train.copy()\n",
    "X_val_SS = X_val.copy()\n",
    "\n",
    "# Convert date column to UNIX timestamp\n",
    "X_train_SS['date'] = pd.to_datetime(X_train_SS['date']).astype('int64') // 10**9\n",
    "X_val_SS['date'] = pd.to_datetime(X_val_SS['date']).astype('int64') // 10**9\n",
    "\n",
    "# Drop unnecessary columns\n",
    "X_train_SS = X_train_SS.drop(columns=[\"user_id\", \"prod_id\", \"review\"], errors='ignore')\n",
    "X_val_SS = X_val_SS.drop(columns=[\"user_id\", \"prod_id\", \"review\"], errors='ignore')\n",
    "\n",
    "# Run SS pipeline for all models\n",
    "for name, model in models.items():\n",
    "    print(f\"=== Model: {name} | Pipeline: SS ===\")\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),  # Standard Scaling\n",
    "        ('smote', SMOTETransformer(sampling_strategy='auto', random_state=42)),  # SMOTE\n",
    "        ('classifier', model)  # Model\n",
    "    ])\n",
    "\n",
    "    # Define scoring metrics\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1': 'f1',\n",
    "        'balanced_accuracy': make_scorer(balanced_accuracy_score),\n",
    "        'mcc': make_scorer(matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    # Perform cross-validation\n",
    "    cv_results = cross_validate(pipeline, X_train_SS, y_train, cv=cv, scoring=scoring, return_train_score=False)\n",
    "\n",
    "    # Get predicted probabilities for PR-AUC & ROC-AUC\n",
    "    y_pred_proba = cross_val_predict(pipeline, X_train_SS, y_train, cv=cv, method=\"predict_proba\")\n",
    "\n",
    "    # Compute additional metrics\n",
    "    roc_auc_0 = roc_auc_score(y_train, y_pred_proba[:, 0])\n",
    "    roc_auc_1 = roc_auc_score(y_train, y_pred_proba[:, 1])\n",
    "\n",
    "    precision_0, recall_0, _ = precision_recall_curve(y_train, y_pred_proba[:, 0])\n",
    "    pr_auc_0 = auc(recall_0, precision_0)\n",
    "\n",
    "    precision_1, recall_1, _ = precision_recall_curve(y_train, y_pred_proba[:, 1])\n",
    "    pr_auc_1 = auc(recall_1, precision_1)\n",
    "\n",
    "    # Store metrics\n",
    "    results.append({\n",
    "        \"Model\": f\"{name}_SS\",\n",
    "        \"Accuracy\": f\"{np.mean(cv_results['test_accuracy']):.4f}\",\n",
    "        \"Precision\": f\"{np.mean(cv_results['test_precision']):.4f}\",\n",
    "        \"Recall\": f\"{np.mean(cv_results['test_recall']):.4f}\",\n",
    "        \"F1 Score\": f\"{np.mean(cv_results['test_f1']):.4f}\",\n",
    "        \"Balanced Accuracy\": f\"{np.mean(cv_results['test_balanced_accuracy']):.4f}\",\n",
    "        \"MCC\": f\"{np.mean(cv_results['test_mcc']):.4f}\",\n",
    "        \"PR-AUC_0\": f\"{pr_auc_0:.4f}\",\n",
    "        \"PR-AUC_1\": f\"{pr_auc_1:.4f}\",\n",
    "        \"ROC-AUC_0\": f\"{roc_auc_0:.4f}\",\n",
    "        \"ROC-AUC_1\": f\"{roc_auc_1:.4f}\"\n",
    "    })\n",
    "\n",
    "# Save results in a single CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"b4_pipeline_comparison_results_SS.csv\", index=False)\n",
    "\n",
    "print(\"SS pipeline models evaluated and results saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
